{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 2: Python and Libraries\n",
        "\n",
        "# Python Basics\n",
        "\n",
        "## 1. Variables and Data Types\n",
        "\n",
        "-   Variables are used to store data values\n",
        "\n",
        "-   Python has several built-in data types:\n",
        "\n",
        "    -   Numeric types: int, float, complex\n",
        "    -   Sequence types: list, tuple, range\n",
        "    -   Text type: str\n",
        "    -   Mapping type: dict\n",
        "    -   Set types: set, frozenset\n",
        "    -   Boolean type: bool\n",
        "\n",
        "-   Variables are created using the assignment operator (=)\n",
        "\n",
        "### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = 5         # int\n",
        "y = 3.14      # float\n",
        "name = \"John\" # str\n",
        "is_student = True  # bool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Operators\n",
        "\n",
        "-   Arithmetic operators: +, -, \\*, /, %, //, \\*\\*\n",
        "\n",
        "-   Comparison operators: ==, !=, \\<, \\>, \\<=, \\>=\n",
        "\n",
        "-   Logical operators: and, or, not\n",
        "\n",
        "-   Assignment operators: =, +=, -=, \\*=, /=, %=, //=, \\*\\*=\n",
        "\n",
        "### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = 5\n",
        "y = 3\n",
        "print(x + y)  # 8\n",
        "print(x > y)  # True\n",
        "print((x > 3) and (y < 5))  # True\n",
        "x += 2  # x is now 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Control Flow\n",
        "\n",
        "-   if, elif, else statements for conditional execution\n",
        "\n",
        "-   for and while loops for iteration\n",
        "\n",
        "-   break, continue, and pass statements for loop control\n",
        "\n",
        "### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = 5\n",
        "if x > 0:\n",
        "    print(\"Positive\")\n",
        "elif x < 0:\n",
        "    print(\"Negative\")\n",
        "else:\n",
        "    print(\"Zero\")\n",
        "for i in range(5):\n",
        "    print(i)\n",
        "i = 0\n",
        "while i < 5:\n",
        "    print(i)\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Functions\n",
        "\n",
        "-   Functions are defined using the def keyword\n",
        "\n",
        "-   They can take parameters and return values\n",
        "\n",
        "### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def greet(name):\n",
        "    print(f\"Hello, {name}!\")\n",
        "def add(x, y):\n",
        "    return x + y\n",
        "greet(\"John\")  # Hello, John!\n",
        "result = add(3, 5)  # result is 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python Data Structures\n",
        "\n",
        "### 1. Lists\n",
        "\n",
        "-   Lists are ordered, mutable sequences\n",
        "\n",
        "-   They are created using square brackets \\[\\]\n",
        "\n",
        "-   Elements can be accessed by index\n",
        "\n",
        "#### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
        "print(fruits[0])  # apple\n",
        "fruits[1] = \"blueberry\"\n",
        "fruits.append(\"date\")\n",
        "print(fruits)  # [\"apple\", \"blueberry\", \"cherry\", \"date\"]\n",
        "\n",
        "# Creating a list of squares of numbers from 1 to 5 using list comprehension\n",
        "squares = [i ** 2 for i in range(1, 6)]\n",
        "print(squares)  # Output: [1, 4, 9, 16, 25]\n",
        "\n",
        "# Filtering even numbers from a list using list comprehension\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "even_numbers = [num for num in numbers if num % 2 == 0]\n",
        "print(even_numbers)  # Output: [2, 4, 6, 8, 10]\n",
        "\n",
        "# Creating a list of strings using list comprehension\n",
        "names = [\"Alice\", \"Bob\", \"Charlie\", \"David\"]\n",
        "upper_case_names = [name.upper() for name in names]\n",
        "print(upper_case_names)  # Output: ['ALICE', 'BOB', 'CHARLIE', 'DAVID']\n",
        "\n",
        "# Rotating names\n",
        "rotated_names = [names[i:]+names[:i] for i in range(4)] \n",
        "print(rotated_names)\n",
        "\n",
        "# More advanced examples\n",
        "# Creating a 4x4 identity matrix using list comprehension\n",
        "identity_matrix = [[1 if i == j else 0 for j in range(4)] for i in range(4)]\n",
        "print(identity_matrix)\n",
        "# Output: [[1, 0, 0, 0],\n",
        "#           [0, 1, 0, 0],\n",
        "#           [0, 0, 1, 0],\n",
        "#           [0, 0, 0, 1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# list all pdf files in the current directory\n",
        "import glob\n",
        "all_files = glob.glob('./*')\n",
        "# print(all_files)\n",
        "\n",
        "pdf_files = [file for file in all_files if file[-4:].lower()=='.pdf']\n",
        "print(pdf_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Iterating over a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generating a list of random numbers and squaring even numbers\n",
        "import random\n",
        "\n",
        "random_numbers = [random.randint(1, 10) for _ in range(10)]\n",
        "squared_even_numbers = [num ** 2 for num in random_numbers if num % 2 == 0]\n",
        "print(random_numbers)\n",
        "print(squared_even_numbers)\n",
        "# Example output: [7, 9, 1, 3, 2, 10, 6, 8, 4, 5]\n",
        "#                 [4, 100, 4, 36]\n",
        "\n",
        "\n",
        "my_list = ['apple', 'banana', 'cherry', 'date']\n",
        "# Using a while loop to iterate over the list\n",
        "index = 0\n",
        "while index < len(my_list):\n",
        "    print(my_list[index])\n",
        "    index += 1\n",
        "\n",
        "# Output:\n",
        "# 1\n",
        "# 2\n",
        "# 3\n",
        "# 4\n",
        "# 5\n",
        "\n",
        "# Using a for loop with index\n",
        "for index in range(len(my_list)):\n",
        "    print(my_list[index])\n",
        "\n",
        "# Output:\n",
        "# 1\n",
        "# 2\n",
        "# 3\n",
        "# 4\n",
        "# 5\n",
        "\n",
        "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
        "for fruit in fruits:\n",
        "    print(fruit)\n",
        "\n",
        "# Output:\n",
        "# apple\n",
        "# banana\n",
        "# cherry\n",
        "\n",
        "my_list = [1, 2, 3, 4, 5]\n",
        "\n",
        "# Using a for loop to iterate over the list\n",
        "for element in my_list:\n",
        "    print(element)\n",
        "\n",
        "# Using enumerate() to iterate over the list with index and value\n",
        "for index, value in enumerate(my_list):\n",
        "    print(f\"Index {index}: {value}\")\n",
        "\n",
        "# Output:\n",
        "# Index 0: apple\n",
        "# Index 1: banana\n",
        "# Index 2: cherry\n",
        "# Index 3: date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Lists and Generators\n",
        "\n",
        "Generators in Python are a memory-efficient way to create iterators. They use the `yield` keyword to produce values on-demand, conserving memory for large or infinite sequences. Generators are defined like functions but use `yield` to generate values dynamically. They are useful for lazy evaluation, infinite sequences, and streaming data.\n",
        "\n",
        "Here’s an extended version of the previous example that includes the use of the `iter()` and `next()` functions, as well as a generator function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List comprehension to square numbers from 1 to 5\n",
        "squared_numbers = [i ** 2 for i in range(1, 6)]\n",
        "print(squared_numbers)  # Output: [1, 4, 9, 16, 25]\n",
        "\n",
        "# Generator expression to square numbers from 1 to 5\n",
        "squared_numbers_generator = (i ** 2 for i in range(1, 6))\n",
        "\n",
        "# Using iter() and next() with the generator expression\n",
        "generator_iterator = iter(squared_numbers_generator)\n",
        "print(next(generator_iterator))  # Output: 1\n",
        "print(next(generator_iterator))  # Output: 4\n",
        "print(next(generator_iterator))  # Output: 9\n",
        "print(next(generator_iterator))  # Output: 16\n",
        "print(next(generator_iterator))  # Output: 25\n",
        "\n",
        "# Generator function to square numbers from 1 to n\n",
        "def square_generator(n):\n",
        "    for i in range(1, n + 1):\n",
        "        yield i ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "iter1 = iter(square_generator(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(next(iter1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Using the generator function\n",
        "generator_from_function = square_generator(5)\n",
        "for squared_number in generator_from_function:\n",
        "    print(squared_number)\n",
        "# Output:\n",
        "# 1\n",
        "# 4\n",
        "# 9\n",
        "# 16\n",
        "# 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this extended example, we first use the `iter()` function to obtain an iterator from the generator expression `squared_numbers_generator`. Then, we use the `next()` function to retrieve the next value from the iterator. We can call `next()` multiple times to get the next values from the generator expression.\n",
        "\n",
        "Additionally, we introduce a generator function `square_generator(n)` that takes an argument `n` and yields the squares of numbers from 1 to `n`. We create a generator object `generator_from_function` by calling the generator function with an argument of 5. We then iterate over the generator object using a `for` loop, which automatically handles calling `next()` and stopping at the `StopIteration` exception.\n",
        "\n",
        "### 2. Tuples\n",
        "\n",
        "-   Tuples are ordered, immutable sequences\n",
        "\n",
        "-   They are created using parentheses ()\n",
        "\n",
        "-   Elements can be accessed by index\n",
        "\n",
        "#### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "point = (3, 5)\n",
        "print(point[0])  # 3\n",
        "x, y = point  # unpacking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Dictionaries\n",
        "\n",
        "-   Dictionaries are unordered, mutable mappings of keys to values\n",
        "\n",
        "-   They are created using curly braces {}\n",
        "\n",
        "-   Elements are accessed by key\n",
        "\n",
        "#### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "person = {\"name\": \"John\", \"age\":15, \"city\": \"New York\"}\n",
        "print(person[\"name\"])  # John\n",
        "person[\"age\"] = 31\n",
        "person[\"email\"] = \"john@example.com\"\n",
        "print(person)  # {\"name\": \"John\", \"age\": 31, \"city\": \"New York\", \"email\": \"john@example.com\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Sets\n",
        "\n",
        "-   Sets are unordered, mutable collections of unique elements\n",
        "\n",
        "-   They are created using curly braces {} or the set() function\n",
        "\n",
        "#### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fruits = {\"apple\", \"banana\", \"cherry\"}\n",
        "fruits.add(\"date\")\n",
        "fruits.remove(\"banana\")\n",
        "print(fruits)  # {\"apple\", \"cherry\", \"date\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python Classes\n",
        "\n",
        "Classes in Python provide a way to define custom data structures that encapsulate data (attributes) and functions (methods) that operate on that data. They are a fundamental concept in object-oriented programming (OOP) and are extensively used in PyTorch for defining neural network models, custom datasets, and more.\n",
        "\n",
        "### Defining a Class\n",
        "\n",
        "A class is defined using the `class` keyword. The class name is a string that identifies the class. Class names typically follow the CamelCase convention. The class definition must be preceded by the `class` keyword, followed by the class name, and a colon. The class definition is followed by a series of class attributes and methods, which are defined using the `def` keyword. Class attributes are variables that are shared by all instances of the class. They are defined within the class but outside any methods. Class methods are functions that are defined within the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MyClass:\n",
        "# Class definition goes here\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Class Attributes\n",
        "\n",
        "Class attributes are variables that are shared by all instances of the class. They are defined within the class but outside any methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MyClass:\n",
        "    class_attr = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(MyClass.class_attr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instance Attributes\n",
        "\n",
        "Instance attributes are unique to each instance of the class. They are typically defined in the `__init__` method, which is a special method called when a new instance of the class is created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MyClass:\n",
        "    def __init__(self, value):\n",
        "        self.instance_attr = value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance1 = MyClass(15)   # calls __init__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance1.instance_attr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `self` parameter refers to the instance being created and is used to access its attributes and methods. \n",
        "\n",
        "### Methods\n",
        "\n",
        "Methods are functions defined within a class that operate on the class’s data. They take `self` as the first parameter, which allows them to access the instance’s attributes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MyClass:\n",
        "    def __init__(self, value):\n",
        "        self.instance_attr = value\n",
        "    \n",
        "    def my_method(self):\n",
        "        print(f\"Instance attribute: {self.instance_attr}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance2 = MyClass(4)\n",
        "instance2.my_method()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inheritance\n",
        "\n",
        "Inheritance allows a class to inherit attributes and methods from another class, called the superclass or base class. The inheriting class is called the subclass or derived class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class BaseClass:\n",
        "    def base_method(self):\n",
        "        print(\"This is the base method.\")\n",
        "class DerivedClass(BaseClass):\n",
        "    def derived_method(self):\n",
        "        print(\"This is the derived method.\")\n",
        "\n",
        "instance3 = DerivedClass()\n",
        "instance3.base_method()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The derived class inherits all attributes and methods from the base class and can also define its own.\n",
        "\n",
        "### Super()\n",
        "\n",
        "The `super()` function allows a subclass to call methods from its superclass, even if the method has been overridden in the subclass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class BaseClass:\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "class DerivedClass(BaseClass):\n",
        "    def __init__(self, value, additional_value):\n",
        "        super().__init__(value)\n",
        "        self.additional_value = additional_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Father:\n",
        "    def __init__(self, father_name):\n",
        "        self.father_name = father_name\n",
        "        print(f\"Father's name: {self.father_name}\")\n",
        "\n",
        "class Mother:\n",
        "    def __init__(self, mother_name):\n",
        "        self.mother_name = mother_name\n",
        "        print(f\"Mother's name: {self.mother_name}\")\n",
        "\n",
        "class Child(Father, Mother):\n",
        "    def __init__(self, father_name, mother_name, child_name):\n",
        "        super().__init__(father_name)  # Initializes Father class\n",
        "        super(Father, self).__init__(mother_name)  # Initializes Mother class\n",
        "        self.child_name = child_name\n",
        "        print(f\"Child's name: {self.child_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Abstract Base Classes\n",
        "\n",
        "Abstract base classes (ABCs) define a common interface for a set of subclasses. They cannot be instantiated directly and may contain abstract methods that must be implemented by the subclasses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from abc import ABC, abstractmethod\n",
        "class MyABC(ABC):\n",
        "    @abstractmethod\n",
        "    def my_abstract_method(self):\n",
        "        pass\n",
        "class MyConcreteClass(MyABC):\n",
        "    def my_abstract_method(self):\n",
        "        print(\"Implementation of the abstract method.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: PyTorch Model Definition\n",
        "\n",
        "In PyTorch, neural network models are defined as classes that inherit from the `nn.Module` base class. The model’s layers are defined as instance attributes in the `__init__` method, and the forward pass is defined in the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden(x))\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "myModel = MyModel(2,3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample_input= torch.Tensor([20,30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "myModel(sample_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example defines a simple feedforward neural network with one hidden layer and ReLU activation.\n",
        "\n",
        "### Example: PyTorch Custom Dataset\n",
        "\n",
        "PyTorch datasets are defined as classes that inherit from the `torch.utils.data.Dataset` base class. The dataset class must implement the `__len__` method, which returns the size of the dataset, and the `__getitem__` method, which returns a single sample from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example defines a simple dataset that wraps a list of data points and their corresponding labels.\n",
        "\n",
        "## Python Libraries\n",
        "\n",
        "### 1. NumPy\n",
        "\n",
        "-   NumPy, short for Numerical Python, is a fundamental library for scientific computing and data analysis in Python. It provides powerful tools for working with multi-dimensional arrays and matrices, along with a vast collection of mathematical functions.\n",
        "\n",
        "#### 1. Installation\n",
        "\n",
        "    Before embarking on your NumPy journey, ensure you have Python installed (preferably Python 3.6 or higher). Installing NumPy is straightforward using pip, the Python package installer:\n",
        "\n",
        "    ``` bash\n",
        "    pip install numpy\n",
        "    ```\n",
        "\n",
        "    If you’re using a Conda environment, use:\n",
        "\n",
        "    ``` bash\n",
        "    conda install numpy\n",
        "    ```\n",
        "\n",
        "#### 2. NumPy Arrays\n",
        "\n",
        "    NumPy’s core data structure is the `ndarray`, a multi-dimensional array capable of holding elements of the same data type.\n",
        "\n",
        "**Creating Arrays:**\n",
        "\n",
        "    -   From Python lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "arr = np.array([1, 2, 3, 4])  # Creates a 1D array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Using built-in functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zeros_arr = np.zeros((2, 3))  # Creates a 2x3 array filled with zeros\n",
        "ones_arr = np.ones((4, 4))    # Creates a 4x4 array filled with ones\n",
        "range_arr = np.arange(10)     # Creates an array with values from 0 to 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Array Attributes:**\n",
        "\n",
        "-   `ndim`: Number of dimensions (e.g., 2 for a matrix)\n",
        "-   `shape`: Tuple representing the size of each dimension (e.g.,\n",
        "    (2, 3) for a 2x3 matrix)\n",
        "-   `size`: Total number of elements in the array\n",
        "-   `dtype`: Data type of the elements (e.g., int32, float64)\n",
        "\n",
        "#### 3. Array Indexing and Slicing\n",
        "\n",
        "**Indexing:**\n",
        "\n",
        "Access individual elements using square brackets and indices:\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "first_element = arr[0]        # Access the first element\n",
        "element_2d = zeros_arr[1, 2]  # Access element at row 1, column 2\n",
        "last_element = arr[-1]       # Access the last element using -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Slicing:**\n",
        "\n",
        "Extract subarrays using slicing syntax:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sub_arr = arr[1:4]           # Elements from index 1 (inclusive) to 4 (exclusive)\n",
        "arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "all_rows_column_1 = arr2d[:, 1]  # All elements in the second column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Shape and Reshaping\n",
        "\n",
        "**Reshaping:**\n",
        "\n",
        "Change the shape of an array without modifying its data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "arr = np.arange(12)\n",
        "new_arr = arr.reshape(3, 4)  # Reshape into a 3x4 matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Mathematical Operations\n",
        "\n",
        "NumPy offers a wide range of mathematical functions:\n",
        "\n",
        "**Element-wise Operations:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "\n",
        "c = a + b  # Element-wise addition\n",
        "d = a * b  # Element-wise multiplication"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Matrix Operations:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "matrix_product = np.dot(a, b)  # Matrix multiplication"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "or equivalently,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "matrix_product = a @ b  # Matrix multiplication, equivalent to np.dot(a, b) in this case"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that `@` is specifically for matrix multiplication, while\n",
        "`np.dot()` can be used for both matrix multiplication and dot\n",
        "product, depending on the input arrays. For 2-D arrays, it is\n",
        "generally preferred to use `@` or `np.matmul()` over `np.dot()`.\n",
        "\n",
        "#### 6. Broadcasting\n",
        "\n",
        "Broadcasting enables operations between arrays of different shapes:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "arr = np.ones((3, 3))\n",
        "arr + 5  # Adds 5 to each element of the array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7. Data Types and Precision\n",
        "\n",
        "NumPy supports various data types, each with different precision and\n",
        "memory requirements:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "int_arr = np.array([1, 2, 3], dtype='int16')  # Array of 16-bit integers\n",
        "float_arr = np.array([1.5, 2.2, 3.8], dtype='float32')  # Array of 32-bit floats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 8. Vectorized Computation\n",
        "\n",
        "NumPy excels at performing operations on entire arrays without\n",
        "explicit loops, leading to significant performance gains:\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "arr = np.arange(10)\n",
        "arr_squared = arr ** 2  # Squares each element in the array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By mastering these core concepts, you’ll be well-equipped to\n",
        "leverage NumPy’s power for efficient numerical computations and data\n",
        "analysis, paving the way for further exploration in data science and\n",
        "machine learning.\n",
        "\n",
        "\n",
        "### 2. Matplotlib\n",
        "\n",
        "-   Matplotlib is a plotting library for creating static, animated, and interactive visualizations\n",
        "\n",
        "#### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "x = np.linspace(0, 2 * np.pi, 100)\n",
        "y = np.sin(x)\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Sine Wave\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"sin(x)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Pandas\n",
        "\n",
        "\n",
        "-   Pandas is a library for data manipulation and analysis\n",
        "\n",
        "-   It provides data structures like Series and DataFrame\n",
        "\n",
        "#### Examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "data = {\"name\": [\"John\", \"Anna\", \"Peter\", \"Linda\"],\n",
        "        \"age\": [35, 28, 42, 33],\n",
        "        \"city\": [\"New York\", \"Paris\", \"Berlin\", \"London\"]}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "print(df[\"age\"].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial covers the basics of Python programming, including\n",
        "variables, data types, operators, control flow, functions, data\n",
        "structures (lists, tuples, dictionaries, sets), and some commonly\n",
        "used libraries (NumPy, Matplotlib, Pandas). It provides a solid\n",
        "foundation for students to start learning Python and prepares them\n",
        "for more advanced topics in machine learning and deep learning.\n",
        "\n",
        "\n",
        "## Three sample python programs\n",
        "\n",
        "In this section, we will implement three sample Python programs.\n",
        "\n",
        "### 1. Simple Perceptron\n",
        "\n",
        "This simple Multi-Layer Perceptron (MLP) demonstrates the basic building block of neural networks. It learns to classify binary inputs using a single hidden layer and a sigmoid activation function. The MLP adjusts its weights and bias through training to approximate the desired output function, in this case, the ** OR ** logic gate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimplePerceptron:\n",
        "    def __init__(self, input_size):\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
        "        return self.sigmoid(weighted_sum)\n",
        "\n",
        "    def train(self, inputs, target, learning_rate):\n",
        "        prediction = self.predict(inputs)\n",
        "        error = target - prediction\n",
        "        gradient = error * prediction * (1 - prediction)\n",
        "        self.weights += learning_rate * gradient * inputs\n",
        "        self.bias += learning_rate * gradient\n",
        "\n",
        "# Example usage\n",
        "input_size = 2\n",
        "perceptron = SimplePerceptron(input_size)\n",
        "\n",
        "# Training data\n",
        "training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "training_targets = np.array([0, 1, 1, 1])\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "learning_rate = 0.1\n",
        "\n",
        "for _ in range(epochs):\n",
        "    for inputs, target in zip(training_inputs, training_targets):\n",
        "        perceptron.train(inputs, target, learning_rate)\n",
        "\n",
        "# Test the perceptron\n",
        "test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "for inputs in test_inputs:\n",
        "    prediction = perceptron.predict(inputs)\n",
        "    print(f\"Input: {inputs}, Prediction: {prediction:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. MLP with Backpropagation\n",
        "\n",
        "This multi-layer perceptron (MLP) introduces the concept of hidden layers and backpropagation. It learns to solve the XOR problem, which cannot be solved by a single-layer perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1, num_epochs=1000):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z = np.dot(X, self.weights1)\n",
        "        self.z2 = sigmoid(self.z)\n",
        "        self.z3 = np.dot(self.z2, self.weights2)\n",
        "        output = sigmoid(self.z3)\n",
        "        return output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        self.output_error = y - output\n",
        "        self.output_delta = self.output_error * sigmoid_derivative(output)\n",
        "        self.z2_error = np.dot(self.output_delta, self.weights2.T)\n",
        "        self.z2_delta = self.z2_error * sigmoid_derivative(self.z2)\n",
        "        self.weights1 += self.learning_rate * np.dot(X.T, self.z2_delta)\n",
        "        self.weights2 += self.learning_rate * np.dot(self.z2.T, self.output_delta)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for _ in range(self.num_epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "# Training data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create and train the MLP\n",
        "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
        "mlp.train(X, y)\n",
        "\n",
        "# Test the MLP\n",
        "print(mlp.predict(np.array([[0, 0]])))  # Output: [[0.01...]]\n",
        "print(mlp.predict(np.array([[1, 1]])))  # Output: [[0.98...]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Logistic Regression\n",
        "\n",
        "In this section, we will implement a logistic regression model to classify binary inputs. First, we remind the Binary Cross Entropy Loss function.\n",
        "\n",
        "#### Binary Cross Entropy Loss Function\n",
        "\n",
        "The binary cross-entropy loss, often used in binary classification tasks, measures the performance of a classification model whose output is a probability value between 0 and 1. The loss quantifies the dissimilarity between the predicted probability distribution and the true binary labels of a dataset.\n",
        "\n",
        "The binary cross-entropy loss for a dataset is calculated as:\n",
        "\n",
        "\n",
        "$L = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$\n",
        "\n",
        "Where:\n",
        "- $N$ is the number of samples.\n",
        "- $y_i$ is the actual label of the ith sample, which can be 0 or 1.\n",
        "- $\\hat{y}_i$ is the predicted probability that the ith sample belongs to class 1.\n",
        "\n",
        "This loss function is particularly effective because it heavily penalizes predictions that are confident and wrong. For instance, if the true label $y_i$ is 1 and the predicted probability $\\hat{y}_i$ is close to 0, the loss becomes very large.\n",
        "\n",
        "The gradient of the binary cross-entropy loss with respect to the weights (denoted as $\\text{dw}$) is essential for updating the weights during the training process using gradient descent. Here's how it is derived:\n",
        "\n",
        "1. **Gradient with respect to the weights $W$**:\n",
        "   The derivative of the loss function with respect to the weights can be derived using the chain rule. The partial derivative of the loss with respect to each weight $w_j$ in the weights vector $W$ is given by:\n",
        "\n",
        "   $\\frac{\\partial L}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i) x_{ij}$\n",
        "\n",
        "   Where $x_{ij}$ is the jth feature of the ith sample.\n",
        "\n",
        "2. **Vectorized Form**:\n",
        "   In a more compact vectorized form, the gradient of the loss with respect to the weights vector $W$ can be expressed as:\n",
        "\n",
        "   $\\text{dw} = \\frac{1}{N} X^T (\\hat{y} - y)$\n",
        "\n",
        "   Here, $X^T$ is the transpose of the matrix of input features, $\\hat{y}$ is the vector of predicted probabilities, and $y$ is the vector of actual labels. This expression shows that the gradient is the average of the product of the input features and the prediction errors across all samples.\n",
        "\n",
        "The weights are updated in the gradient descent step as follows:\n",
        "\n",
        "$W = W - \\eta \\cdot \\text{dw}$\n",
        "\n",
        "Where $\\eta$ is the learning rate, a hyperparameter that determines the step size at each iteration in the gradient descent.\n",
        "\n",
        "By iteratively applying this update rule, the logistic regression model adjusts its weights to minimize the binary cross-entropy loss, thereby improving its accuracy in predicting the class labels.\n",
        "\n",
        "\n",
        "For more details see [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) and [Derivation of the Binary Cross Entropy Loss Gradient](https://www.python-unleashed.com/post/derivation-of-the-binary-cross-entropy-loss-gradient).\n",
        "\n",
        "#### The Logistic Regression Model\n",
        "\n",
        "logistic regression is a powerful and widely used statistical method for binary classification problems. It models the probability of an event based on input variables using the sigmoid function and a linear combination of the predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    \"\"\"Compute the sigmoid of x.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the Logistic Regression model class\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        \"\"\"Initialize the logistic regression model with specified learning rate and number of iterations.\"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the logistic regression model to the training data.\"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient descent to optimize weights and bias\n",
        "        for _ in range(self.num_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = sigmoid(linear_model)\n",
        "\n",
        "            # # Binary cross-entropy loss is used here for binary classification\n",
        "            # loss = -(1 / num_samples) * np.sum(\n",
        "            #     y * np.log(y_predicted) + (1 - y) * np.log(1 - y_predicted)\n",
        "            # )\n",
        "\n",
        "            # Derivative of binary cross-entropy loss\n",
        "            # z = linear_model(x) = w * x + b\n",
        "            # y_predicted = sigmoid(z) = 1/(1 + exp(-z))\n",
        "            # dloss/dw = dloss/dy_predicted * dy_predicted/dz * dz/dw\n",
        "            # dloss/dy_predicted = -y / y_predicted + (1 - y) / (1 - y_predicted)\n",
        "            # dy_predicted/dw = dy_predicted/dz * dz/dw\n",
        "            # dy_predicted/dz = y_predicted * (1 - y_predicted)\n",
        "            # dz/dw = x\n",
        "            # Therefore, we have, \n",
        "            # dloss/dw = y_predicted - y    \n",
        "            # For more details, see [Derivation of the Binary Cross Entropy Loss Gradient]\n",
        "            # (https://www.python-unleashed.com/post/derivation-of-the-binary-cross-entropy-loss-gradient).\n",
        "            \n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Update weights and bias\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict binary labels for a batch of inputs.\"\"\"\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = sigmoid(linear_model)\n",
        "        return np.array([1 if i > 0.5 else 0 for i in y_predicted])\n",
        "\n",
        "# Generate a random dataset\n",
        "np.random.seed(43)\n",
        "X = np.random.randn(100, 2)\n",
        "y = np.array([1 if x1 + x2 > 0 else 0 for x1, x2 in X])\n",
        "\n",
        "# save indices where y==1\n",
        "idx_class1 = np.where(y == 1)[0]\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions on new data\n",
        "X_test = np.array([[1, 1], [-1, -1], [2, 2], [-2, -2]])\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Plot the dataset and decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.scatter(X[~idx_class1, 0], X[~idx_class1, 1], c='blue', cmap='bwr', edgecolors='black', s=50, label='Class 0')\n",
        "\n",
        "plt.scatter(X[idx_class1, 0], X[idx_class1, 1], c='red', cmap='bwr', edgecolors='white', s=50, label='Class 1')\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='bwr', edgecolors='black', s=75, marker='s', label='Test Points')\n",
        "\n",
        "# Calculate limits and create a meshgrid for contour plot\n",
        "x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))\n",
        "# Here, np.c_ concatenates the flattened (ravel()) versions of xx1 and xx2 arrays along \n",
        "# the second axis, effectively creating a list of [x1, x2] pairs over which the model's \n",
        "# predict method is called. \n",
        "Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n",
        "plt.contourf(xx1, xx2, Z, alpha=0.3, cmap='bwr')\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlim(x1_min, x1_max)\n",
        "plt.ylim(x2_min, x2_max)\n",
        "plt.xlabel('Feature 1 (x1)')\n",
        "plt.ylabel('Feature 2 (x2)')\n",
        "plt.title('Logistic Regression Classification with Decision Boundary')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ![The Output](images/output03.png) -->\n",
        "\n",
        "\n",
        "In this example, we define a logistic regression model using only NumPy. The model is trained using the fit method, which performs gradient descent to optimize the weights and bias. The predict method is used to make predictions on new data.\n",
        "\n",
        "We generate a random dataset with two features and binary labels, where the labels are determined by a simple decision rule (x1 + x2 \\> 0). The logistic regression model is trained on this dataset.\n",
        "\n",
        "After training, we make predictions on a few test points to demonstrate the model’s performance.\n",
        "\n",
        "Finally, we use Matplotlib to visualize the dataset and the decision boundary learned by the logistic regression model. The training data points are plotted with different colors based on their class labels. The test points are plotted as squares with black edges. The decision boundary is represented by the contour plot, which separates the two classes.\n",
        "\n",
        "The plot shows the training data points, test points, and the decision boundary learned by the logistic regression model. The model has successfully learned to separate the two classes based on the given features.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}